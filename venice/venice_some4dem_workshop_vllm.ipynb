{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8c36800-5d6d-469c-81d5-d2b85983920a",
   "metadata": {},
   "source": [
    "# Venice some4dem workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6903819-98ba-4d74-9ab1-01b58a935812",
   "metadata": {},
   "source": [
    "## vLLM inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff007628-950b-4930-ad13-6fe99cd6c352",
   "metadata": {},
   "source": [
    "### (1) Promts preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f14dee19-0ac0-4f00-ab91-b91a6c9a9922",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from string import Template\n",
    "\n",
    "from essential_generators import DocumentGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8e307bd-3112-40d1-b9d7-492fe9635728",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_content = \"You are an expert in politics.\"\n",
    "\n",
    "instructions = \"\"\"Please classify the following social media profile according\n",
    "to whether it expresses support or positive attitudes towards  French righ wing parties.\n",
    "Here is the message: '${text}'\"\"\"\n",
    "\n",
    "def make_promts(text):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_content},\n",
    "        {\"role\": \"user\", \"content\": Template(instructions).substitute(text=text)},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "664aba64-8b41-40cc-a14b-583144a43363",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"\"\"Gaulliste/souverainet√© nationale et populaire.\n",
    "Cofondateur du Mouvement Politique Citoyen.\n",
    "Rejoignez-nous, Adh√©rer ‚§µÔ∏è, Aidez-nous √† sauver la France !\"\"\"\n",
    "\n",
    "text2 = \"\"\"Le coeur qui bat √† gauche ‚Ä¢ Avec @egregoire\n",
    " pour un @paris_en_grand\n",
    " ‚Ä¢ Anime @coeurgauche75\n",
    " ‚Ä¢üó£Ô∏èüá™üá∫ au @partisocialiste\n",
    " et üá´üá∑au @pes_pse\n",
    " ‚Ä¢ Militant @psparis10\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92a961b1-c911-493d-b9d6-bb95b3f0fc1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system', 'content': 'You are an expert in politics.'},\n",
       " {'role': 'user',\n",
       "  'content': \"Please classify the following social media profile according\\nto whether it expresses support or positive attitudes towards  French righ wing parties.\\nHere is the message: 'Gaulliste/souverainet√© nationale et populaire.\\nCofondateur du Mouvement Politique Citoyen.\\nRejoignez-nous, Adh√©rer ‚§µÔ∏è, Aidez-nous √† sauver la France !'\"}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_promts(text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434fb13f-3856-4751-8899-11c55ac69f06",
   "metadata": {},
   "source": [
    "### (2) vllM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d45f69d0-96ff-4849-8ee0-765f88e159b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jul  7 00:04:25 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.05              Driver Version: 560.35.05      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX A4500               Off |   00000000:0A:00.0 Off |                  Off |\n",
      "| 35%   63C    P0             83W /  200W |       1MiB /  20470MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA RTX A4500               Off |   00000000:AE:00.0 Off |                  Off |\n",
      "| 36%   59C    P2             80W /  200W |   15601MiB /  20470MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    1   N/A  N/A     74524      C   ...sions/3.12.7/envs/venice/bin/python      15592MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450e8bc9-32a9-4a42-8630-e797cc7ad20c",
   "metadata": {},
   "source": [
    "### (2.1) Load the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd13037a-46e0-4b32-b76c-aa08b7b9ad60",
   "metadata": {},
   "source": [
    "#### First attempt fails because of memory problems"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8d1a2fe1-23f9-4876-abf0-339d04e1cb5c",
   "metadata": {},
   "source": [
    "from vllm import LLM\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "llm = LLM(\n",
    "    model=model_id,\n",
    "    dtype=\"bfloat16\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "85c57a18-46ac-474e-9804-e224a88e134a",
   "metadata": {},
   "source": [
    "ValueError: To serve at least one request with the models's max seq len (131072), (16.00 GiB KV cache is needed, which is larger than the available KV cache memory (1.44 GiB). Based on the available memory, the estimated maximum model length is 11744. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d89d21-6101-4ef7-adb6-a8b39cbaef30",
   "metadata": {},
   "source": [
    "#### Reduce model context length to fit it in memory  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e87e5f3-5adb-4da1-9d12-cf2d6d1181e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-07 00:04:51 [__init__.py:244] Automatically detected platform cuda.\n",
      "INFO 07-07 00:05:01 [config.py:823] This model supports multiple tasks: {'score', 'classify', 'embed', 'reward', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 07-07 00:05:01 [config.py:2195] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 07-07 00:05:04 [core.py:455] Waiting for init message from front-end.\n",
      "INFO 07-07 00:05:04 [core.py:70] Initializing a V1 LLM engine (v0.9.1) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=10000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "WARNING 07-07 00:05:04 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x77cb84b28860>\n",
      "INFO 07-07 00:05:06 [parallel_state.py:1065] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 07-07 00:05:06 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 07-07 00:05:06 [gpu_model_runner.py:1595] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "INFO 07-07 00:05:06 [gpu_model_runner.py:1600] Loading model from scratch...\n",
      "INFO 07-07 00:05:06 [cuda.py:252] Using Flash Attention backend on V1 engine.\n",
      "INFO 07-07 00:05:07 [weight_utils.py:292] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90376b7180c44c009288133ecec30674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-07 00:05:10 [default_loader.py:272] Loading weights took 2.61 seconds\n",
      "INFO 07-07 00:05:11 [gpu_model_runner.py:1624] Model loading took 14.9889 GiB and 4.070573 seconds\n",
      "INFO 07-07 00:05:17 [backends.py:462] Using cache directory: /home/jimena.royoletelier/storage/torch_compile_cache/1fd98ba2be/rank_0_0 for vLLM's torch.compile\n",
      "INFO 07-07 00:05:17 [backends.py:472] Dynamo bytecode transform time: 5.82 s\n",
      "INFO 07-07 00:05:23 [backends.py:135] Directly load the compiled graph(s) for shape None from the cache, took 5.599 s\n",
      "INFO 07-07 00:05:24 [monitor.py:34] torch.compile takes 5.82 s in total\n",
      "INFO 07-07 00:05:27 [gpu_worker.py:227] Available KV cache memory: 1.44 GiB\n",
      "INFO 07-07 00:05:27 [kv_cache_utils.py:715] GPU KV cache size: 11,744 tokens\n",
      "INFO 07-07 00:05:27 [kv_cache_utils.py:719] Maximum concurrency for 10,000 tokens per request: 1.17x\n",
      "INFO 07-07 00:05:54 [gpu_model_runner.py:2048] Graph capturing finished in 26 secs, took 0.51 GiB\n",
      "INFO 07-07 00:05:54 [core.py:171] init engine (profile, create kv cache, warmup model) took 43.09 seconds\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "llm = LLM(\n",
    "    model=model_id,\n",
    "    dtype=\"bfloat16\",\n",
    "    max_model_len=10000\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e065a6b-1a8e-445b-8295-cd42392f879c",
   "metadata": {},
   "source": [
    "### (2.2) Monitor the gpu card memory usage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e40c09f4-b93d-46e8-b038-10e2713cd74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jul  7 00:06:09 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.05              Driver Version: 560.35.05      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX A4500               Off |   00000000:0A:00.0 Off |                  Off |\n",
      "| 37%   67C    P2            105W /  200W |   18859MiB /  20470MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA RTX A4500               Off |   00000000:AE:00.0 Off |                  Off |\n",
      "| 32%   56C    P2             75W /  200W |   15601MiB /  20470MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A     75326      C   ...sions/3.12.7/envs/venice/bin/python      18850MiB |\n",
      "|    1   N/A  N/A     74524      C   ...sions/3.12.7/envs/venice/bin/python      15592MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7330ffb-671c-428d-bbc2-1daeacf3ee3a",
   "metadata": {},
   "source": [
    "### (3) Request the model answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb8bad3-ac4d-4842-9b4c-82c200434c38",
   "metadata": {},
   "source": [
    "#### (3.1) Creates a sampling parameters object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "403bbfc7-318b-4ff5-9390-739b09f7b42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import SamplingParams\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.6,\n",
    "    top_p=0.9, # The model will only consider the results of the tokens with top_p 90% probability mass.\n",
    "    max_tokens=32 # ~ 0.75 * 32 = 24 words\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918ced2e-7129-4180-95eb-7d1484496cd4",
   "metadata": {},
   "source": [
    "#### (3.1) A single answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf15ec89-5d21-4d80-bce0-bbac4cb23dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-07 00:06:29 [chat_utils.py:420] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b39d687af6d4f39a92bf6204d3fd3c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5382f2b7b54f4eb9b41487b90b8836dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                           | 0/1 [00:0‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start = time.time()\n",
    "outputs = llm.chat(messages=make_promts(text1), sampling_params=sampling_params)\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e46e204e-dc43-44d9-9b83-6621351f4e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 2.4875199794769287 seconds.\n",
      "\n",
      "OUTPUT:\n",
      "\t[RequestOutput(request_id=0, prompt=None, prompt_token_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 2675, 527, 459, 6335, 304, 11759, 13, 128009, 128006, 882, 128007, 271, 5618, 49229, 279, 2768, 3674, 3772, 5643, 4184, 198, 998, 3508, 433, 61120, 1862, 477, 6928, 33726, 7119, 220, 8753, 436, 1108, 20611, 9875, 627, 8586, 374, 279, 1984, 25, 364, 81888, 620, 17194, 2754, 15170, 467, 295, 978, 7140, 1604, 1880, 2477, 74775, 627, 34, 1073, 263, 1045, 324, 3930, 73934, 7986, 16307, 2428, 18002, 2303, 268, 627, 697, 7453, 625, 10333, 5392, 788, 11, 2467, 71, 52424, 2928, 97, 113, 31643, 11, 362, 579, 89, 5392, 788, 3869, 33254, 424, 1208, 9822, 758, 6, 128009, 128006, 78191, 128007, 271], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='Based on the content of the message, I would classify this social media profile as expressing support for the French right-wing parties, specifically the Gaullist movement and', token_ids=[29815, 389, 279, 2262, 315, 279, 1984, 11, 358, 1053, 49229, 420, 3674, 3772, 5643, 439, 37810, 1862, 369, 279, 8753, 1314, 29480, 9875, 11, 11951, 279, 18879, 620, 380, 7351, 323], cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})]\n",
      "\n",
      "ANSWER:\n",
      "\tBased on the content of the message, I would classify this social media profile as expressing support for the French right-wing parties, specifically the Gaullist movement and\n"
     ]
    }
   ],
   "source": [
    "answer = outputs[0].outputs[0].text\n",
    "print(f\"Took {end - start} seconds.\\n\")\n",
    "print(f\"OUTPUT:\\n\\t{outputs}\\n\")\n",
    "print(f\"ANSWER:\\n\\t{answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e65589d4-ae22-4076-87fe-fcf8c6b6541b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.49 seconds for one request means 63.7 days per 5.5 millons of requests.\n"
     ]
    }
   ],
   "source": [
    "print(f\"{end - start:.2f} seconds for one request means {1 * 5.5e6 / (24 * 60 * 60):.1f} days per 5.5 millons of requests.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c8208f-3cd0-46e4-b9f0-18bab930dbcf",
   "metadata": {},
   "source": [
    "#### (3.2) Many answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce6ef8c4-febf-4caf-b622-d516f7d0712e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import Template\n",
    "\n",
    "nb_texts = 1000\n",
    "gen = DocumentGenerator()\n",
    "list_of_texts = [gen.sentence() for _ in range(nb_texts)]\n",
    "list_of_messages = [make_promts(text) for text in list_of_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39abfeee-1e54-4a19-b688-19388f6568fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean text lenght is 59.538 words.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean text lenght is {sum([len(t) for t in list_of_texts]) / nb_texts} words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb9e2394-60ec-4210-9955-1625a88c3659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "355a9b32447a49f1be00ea6adbd9ff22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad5d50e2be6b4d4e89d0ec8a74cdb50e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                        | 0/1000 [00:0‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start = time.time()\n",
    "outputs = llm.chat(messages=list_of_messages, sampling_params=sampling_params)\n",
    "took  = time.time() - start\n",
    "per_request_time = took / nb_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "084e90d5-1bad-4886-9bbc-3aab93bd0ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 0.02 seconds per request.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Took {per_request_time:.2f} seconds per request.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d585e2a-b017-49d1-a402-b10f6bc8530f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02 seconds per request means 1.2 days per 5.5 millons of requests.\n"
     ]
    }
   ],
   "source": [
    "print(f\"{per_request_time:.2f} seconds per request means {per_request_time * 5.5e6 / (24 * 60 * 60):.1f} days per 5.5 millons of requests.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b801c15-a332-4aac-82e0-6a54c2d01c0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
